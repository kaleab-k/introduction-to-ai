class: middle, center, title-slide

# Introduction to Artificial Intelligence

Lecture 1: Machine Learning

<br><br>
Kaleab A. Kinfu<br>
[kinfu@jhu.edu](mailto:kinfu@jhu.edu)

???

R: develop linear regression / OLS  further ---> up to the analytical solution

---

# Today

.center.width-50[![](figures/lec1/sl-cartoon.png)]

Make our agents capable of self-improvement through a **learning** mechanism.
- Machine Learning Problems 
- Machine Learning Process
- Machine Learning Tools 
- Machine Learning Mathematics 
- Machine Learning Resources

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

---

class: middle

# Machine learning

---

class: middle

.center[
.width-40[![](figures/lec1/cat.jpg)] &nbsp; &nbsp;
.width-40[![](figures/lec1/dog.jpg)]
]

.exercise[How would you write a computer program that recognizes cats from dogs?]

---

class: middle

.center.width-60[![](figures/lec1/cat1.png)]

---


class: black-slide, middle

.center.width-50[![](figures/lec1/cat2.png)]

.center[The good old-fashioned approach.]

---


class: black-slide, middle

.center.width-80[![](figures/lec1/cat3.png)]

---


class: black-slide, middle

.center.width-80[![](figures/lec1/cat4.png)]

---

class: middle

.center.width-100[![](figures/lec1/catordog-flow.gif)]

.center[The deep learning approach.]

---

class: middle
# What is Machine Learning 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Def.png)]
---
# What is Machine Learning
<br/> 
.slide_left.width-110[![](figures/lec1/ML_Def_2.png)]
---
# What is Machine Learning 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Def_3.png)]
---
# Machine Learning: Why?
<br/>
<br/>
.slide_left.width-110[![](figures/lec1/ML_Why.png)]
---
# Programming vs ML
.slide_left.width-110[![](figures/lec1/ProgVsML.png)]
---

# What is Machine Learning

## ML: What is it good for?

- *Problems with longs lists of rules* 
    - when the traditional approach fails, machine learning may help.
- *Continually changing environments* 
    - machine learning can learn and adapt to new scenarios. 
- *Discovering insights within large collections of data* 
    - can you imagine trying to go through every transaction your (large) company has ever had by hand?

---

# What is Machine Learning

## ML: Terminology

- *Algorithms:* 
    - A set of rules and statistical techniques used to learn patters from data.
- *Model:* 
    - A model is what is trained using an ML algorithm.
- *Predictor Variable:* 
    - It is a feature(s) of the data that can be used to predict the output. 
- *Response Variable:* 
    - It is the output variable that needs to be predicted by using the predictor variables.
- *Training Data:* 
    - The data that is used to build the ML model. 
- *Testing Data:* 
    - The data that is used to evaluate the generalization of the trained ML model.   

---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Process.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_1.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_2.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_3.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_4.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_5.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_6.png)]
---
# Machine Learning Process 
<br/>
.slide_left.width-110[![](figures/lec1/ML_Step_7.png)]
---
# Types of ML: Categories of Learning
<br/>
.slide_left.width-110[![](figures/lec1/ML Types.png)]
---
# Types of ML: Categories of Learning
<br/>
.slide_left.width-110[![](figures/lec1/Supervised.png)]
---
# Types of ML: Categories of Learning
<br/>
.slide_left.width-110[![](figures/lec1/Unsupervised.png)]
---
# Types of ML: Categories of Learning
<br/>
.slide_left.width-110[![](figures/lec1/Reinforcement.png)]
---
# Types of ML: Problem Domains
<br/>
.slide_left.width-110[![](figures/lec1/ML Types_2.png)]
---
# Types of ML: Problem Domains
<br/>
.slide_left.width-110[![](figures/lec1/Regression.png)]
---
# Types of ML: Problem Domains
<br/>
.slide_left.width-110[![](figures/lec1/Classification.png)]
---
# Types of ML: Problem Domains
<br/>
.slide_left.width-110[![](figures/lec1/Clustering.png)]

---

# Problem statement

Let us assume data $\mathbf{d} \sim p(\mathbf{x}, y)$ of $N$ example input-output pairs
    $$\mathbf{d} = \\\{ (\mathbf{x}\_1, y\_1), (\mathbf{x}\_2, y\_2), ..., (\mathbf{x}\_N, y\_N) \\\},$$
where
$\mathbf{x}\_i$ are the input data and
$y_i$ was generated by an unknown function $y\_i=f(\mathbf{x}\_i)$.

From this data, we want to find a function $h \in \mathcal{H}$ that approximates the true function $f$.

???

$\mathcal{H}$ is huge! How do we find a good hypothesis?

---

class: middle

.center.width-10[![](figures/lec1/latent.svg)]

In general, $f$ will be **stochastic**. In this case, $y$ is not strictly a function $x$, and we wish to learn the conditional $p(y|\mathbf{x})$.

Most of supervised learning is actually (approximate) maximum likelihood estimation on (huge) parametric models.

---

class: middle

## Feature vectors

- Input samples $\mathbf{x} \in \mathbb{R}^d$ are described as real-valued vectors of $d$ attributes or features values.
- If the data is not originally expressed as real-valued vectors, then it needs to be prepared and transformed to this format.
.center.width-90[![](figures/lec1/features.png)]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

---

class: center

# ML Tools: scikit-learn

.center.width-60[![](figures/lec1/sklearn-docs.png)]

<a href="http://scikit-learn.org/" style="color:black; font-size:50px; text-decoration:None" >scikit-learn.org</a>

---
class: center

# Representing Data

.center.width-100[![](figures/lec1/matrix-representation.png)]

---
class: center

# Training and Test Data

.center.width-80[![](figures/lec1/train-test-split.png)]

---
class: center, middle

# Notebook: Data Loading

---
class: center

# Linear Models

## For classification and regression.

- Simple models, easy to understand and fast to train.

- Linear models are easy to understand and fast to train.
    - They are typically good baselines.

We will cover intuitions on how they work in a machine learning 
settings.

---

# Linear regression

Let us first assume that $y \in \mathbb{R}$.

<br>
.center.width-90[![](figures/lec1/lr-cartoon.png)]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

---

class: middle

.grid[
.kol-1-5[.center.width-50[![](figures/lec1/latent.svg)]]
.kol-4-5[.center.width-60[![](figures/lec1/lr-latent.png)]]
]


Linear regression considers a parameterized linear Gaussian model for its parametric model of $p(y|\\mathbf{x})$, that is
$$p(y|\mathbf{x}) = \mathcal{N}(y | \mathbf{w}^T \mathbf{x} + b, \sigma^2),$$
where $\mathbf{w}$ and $b$ are parameters to determine.

---

<br><br>

To learn the conditional distribution $p(y|\mathbf{x})$, we maximize
$$p(y|\mathbf{x}) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(y-(\mathbf{w}^T \mathbf{x} + b))^2}{2\sigma^2}\right)$$
w.r.t. $\mathbf{w}$ and $b$ over the data $\mathbf{d} = \\\{ (\mathbf{x}\_j, y\_j) \\\}$.

--




By constraining the derivatives of the log-likelihood to $0$, we arrive to the problem of minimizing
$$\sum\_{j=1}^N (y\_j - (\mathbf{w}^T \mathbf{x}\_j + b))^2.$$
Therefore, minimizing the sum of squared errors corresponds to the MLE solution for a linear fit, assuming Gaussian noise of fixed variance.

---

class: middle

.center.width-80[![](figures/lec1/lq.png)]

---

# Linear classification

Let us now assume $y \in \\{0,1\\}$.

<br>
.center.width-50[![](figures/lec1/classif-cartoon.png)]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]


---

class: middle

The linear classifier model is a squashed linear function of its inputs:
$$h(\mathbf{x}; \mathbf{w}, b) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)$$


.center.width-60[![](figures/lec1/activation-sign.png)]

---

class: middle

.center.width-30[![](figures/lec1/linear-classifier.png)]

- Without loss of generality, the model can be rewritten without $b$ as $h(\mathbf{x}; \mathbf{w}) = \text{sign}(\mathbf{w}^T \mathbf{x})$, where $\mathbf{w} \in \mathbb{R}^{d+1}$ and $\mathbf{x}$ is extended with a dummy element $x\_0 = 1$.
- Predictions are computed by comparing the feature vector $\mathbf{x}$ to the weight vector $\mathbf{w}$. Geometrically, $\mathbf{w}^T \mathbf{x}$ corresponds to $||\mathbf{w}|| ||\mathbf{x}|| \cos(\theta)$.

???

The family $\mathcal{H}$ of hypothesis is induced from the set $\mathbb{R}^{d+1}$ of possible parameters values $\mathbf{w}$ . Learning consists in finding a good vector $\mathbf{w}$ in this space.

---
# Apprenticeship

Can we learn to play Pacman only from observations?
- Feature vectors $\mathbf{x} = g(s)$ are extracted from the game states $s$. Output values $y$ corresponds to actions $a$ .
- State-action pairs $(\mathbf{x}, y)$ are collected by observing an expert playing.
- We want to learn the actions that the expert would take in a given situation. That is, learn the mapping $f:\mathbb{R}^d \to \mathcal{A}$.
- This is a multiclass classification problem.

<br>
.center.width-70[![](figures/lec1/pacman.png)]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

???

<span class="Q">[Q]</span> How is this (very) different from reinforcement learning?

---

class: middle, black-slide

.center[
<video controls muted preload="auto" height="400" width="640">
  <source src="./figures/lec1/training1.mp4" type="video/mp4">
</video>

The Perceptron agent observes a very good Minimax-based agent for two games and updates its weight vectors as data are collected.
]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

---

class: middle, black-slide

.center[
<video controls muted preload="auto" height="400" width="640">
  <source src="./figures/lec1/training2.mp4" type="video/mp4">
</video>

<br><br>]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]

---

class: middle, black-slide

.center[
<video controls muted preload="auto" height="400" width="640">
  <source src="./figures/lec1/apprentice.mp4" type="video/mp4">
</video>

After two training episodes, the Perceptron agents plays.<br>
No more Minimax!
]

.footnote[Image credits: [CS188](https://inst.eecs.berkeley.edu/~cs188/), UC Berkeley.]
